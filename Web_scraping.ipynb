{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNQDlJcUQDo4nw6RyIPlCm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BENMEZIAN/Google-colab/blob/main/Web_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrape elements of **https://dz.kompass.com/x/producer/a/engrais-organiques/22250/**"
      ],
      "metadata": {
        "id": "B-qitIbST9KE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Send an HTTP request to the URL\n",
        "url = \"https://dz.kompass.com/x/producer/a/engrais-organiques/22250/\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract company listings\n",
        "    company_listings = soup.find_all('div', class_='prod_list')\n",
        "\n",
        "    # Open a CSV file in write mode\n",
        "    with open('company_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        # Define CSV writer\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=[\"Company Name\", \"Commune\", \"Wilaya\", \"Summary\", \"Supplies\", \"Phone\", \"Email\", \"Website\"])\n",
        "\n",
        "        # Write header\n",
        "        writer.writeheader()\n",
        "\n",
        "        # Loop through each company listing and extract information\n",
        "        for company_listing in company_listings:\n",
        "            # Extract company name\n",
        "            company_name = company_listing.find('span', class_='titleSpan').text.strip()\n",
        "\n",
        "            # Extract location (commune and wilaya)\n",
        "            location_span = company_listing.find('span', class_='placeText')\n",
        "            location_text = location_span.text.strip()\n",
        "            commune, wilaya = location_text.split(' - ')\n",
        "\n",
        "            # Extract summary\n",
        "            summary = company_listing.find('span', class_='product-summary').text.strip()\n",
        "\n",
        "            # Extract supplies\n",
        "            supplies = [li.text.strip() for li in company_listing.find('ul').find_all('li')]\n",
        "\n",
        "            # Extract phone number\n",
        "            phone = company_listing.find('a', class_='showMobile').text.strip()\n",
        "\n",
        "            # Extract email (if available)\n",
        "            email = \"Unknown\"  # Not found in provided HTML snippet\n",
        "\n",
        "            # Extract website\n",
        "            website_anchor = company_listing.find('a', href=True)\n",
        "            website = website_anchor['href'] if website_anchor else \"Website not found\"\n",
        "\n",
        "            # Write row to CSV file\n",
        "            writer.writerow({\"Company Name\": company_name,\n",
        "                             \"Commune\": commune,\n",
        "                             \"Wilaya\": wilaya,\n",
        "                             \"Summary\": summary,\n",
        "                             \"Supplies\": ', '.join(supplies),\n",
        "                             \"Phone\": phone,\n",
        "                             \"Email\": email,\n",
        "                             \"Website\": website})\n",
        "\n",
        "    print(\"Data has been saved to company_data.csv\")\n",
        "else:\n",
        "    print(\"Failed to retrieve data from the URL:\", url)"
      ],
      "metadata": {
        "id": "NWsR2h6rUPjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrape elements of **https://www.pagesmaghreb.com/entreprises/chimie-et-pharmacie/engrais-et-fertilisants-production/algerie**"
      ],
      "metadata": {
        "id": "XaweHCz1ilAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to extract data from each page\n",
        "def extract_data_from_page(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        company_divs = soup.find_all('div', class_='relative md:flex items-center bg-white rounded-md border shadow mb-4 max-w-screen-md')\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for company_div in company_divs:\n",
        "            company_name = company_div.find('h2', class_='transition text-lg font-bold tracking-tight text-gray-700 hover:text-gray-900').text.strip()\n",
        "            location = company_div.find('span', class_='flex items-center justify-center md:justify-start text-xs text-gray-600 font-bold').text.strip()\n",
        "            function = company_div.find_all('p', class_='font-normal text-xs text-gray-600')[0].text.strip()\n",
        "            prestation = company_div.find_all('p', class_='font-normal text-xs text-gray-600')[1].text.strip()\n",
        "\n",
        "            results.append({\n",
        "                'nom': company_name,\n",
        "                'commune - wilaya': location,\n",
        "                'fonction': function,\n",
        "                'prestation': prestation\n",
        "            })\n",
        "\n",
        "        return results\n",
        "    else:\n",
        "        print(f\"Failed to fetch data from URL: {url}\")\n",
        "        return []\n",
        "\n",
        "# URLs of the pages\n",
        "urls = [\n",
        "    \"https://www.pagesmaghreb.com/entreprises/chimie-et-pharmacie/engrais-et-fertilisants-production/algerie?&page=1\",\n",
        "    \"https://www.pagesmaghreb.com/entreprises/chimie-et-pharmacie/engrais-et-fertilisants-production/algerie?&page=2\",\n",
        "    \"https://www.pagesmaghreb.com/entreprises/chimie-et-pharmacie/engrais-et-fertilisants-production/algerie?&page=3\"\n",
        "]\n",
        "\n",
        "# Extract data from each page\n",
        "all_results = []\n",
        "for url in urls:\n",
        "    all_results.extend(extract_data_from_page(url))\n",
        "\n",
        "# Write the results to a CSV file\n",
        "with open('companies_data.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    fieldnames = ['nom', 'commune - wilaya', 'fonction', 'prestation']\n",
        "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    for result in all_results:\n",
        "        writer.writerow(result)\n",
        "\n",
        "print(\"Data extraction and writing to CSV completed successfully.\")"
      ],
      "metadata": {
        "id": "tqT-Ht4Z_MpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrape elements of **http://www.made-in-algeria.com/data/et_recherche.php?mode_recherche_et=ps&id_ps=204373**"
      ],
      "metadata": {
        "id": "PZ3wSSZhfwCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import csv\n",
        "\n",
        "# Send a GET request to the webpage\n",
        "url = \"http://www.made-in-algeria.com/data/et_recherche.php?mode_recherche_et=ps&id_ps=204373\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find all the tables with the specified attributes\n",
        "    tables = soup.find_all('table', attrs={'width': '98%', 'border': '0', 'cellpadding': '0', 'cellspacing': '0', 'bgcolor': '#FFFFFF'})\n",
        "\n",
        "    # Extract data from each table\n",
        "    for table in tables:\n",
        "        # Find the table rows\n",
        "        rows = table.find_all('tr')\n",
        "\n",
        "        # Extract information from the first row\n",
        "        first_row = rows[0]\n",
        "        company_name = first_row.find('b', class_='tsize14').text.strip()\n",
        "\n",
        "        # Extract information from the second row\n",
        "        second_row = rows[1]\n",
        "        address = second_row.find('td').text.strip()\n",
        "        telephone = second_row.find_all('td')[1].text.strip().split(' - ')[0]\n",
        "        fax = second_row.find_all('td')[1].text.strip().split(' - ')[1]\n",
        "\n",
        "        # Extract products from the third row\n",
        "        third_row = rows[2]\n",
        "        products = third_row.find('td', class_='txt_gray1').text.strip()\n",
        "\n",
        "        # Write the extracted information to a CSV file\n",
        "        with open('company_info.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow(['Company Name', 'Address', 'Telephone', 'Fax', 'Products'])\n",
        "            writer.writerow([company_name, address, telephone, fax, products])\n",
        "\n",
        "        print(\"Data written to company_info.csv successfully.\")\n",
        "else:\n",
        "    print(\"Failed to retrieve the webpage\")"
      ],
      "metadata": {
        "id": "SbTFfNl4gJDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrape elements of **https://www.goafricaonline.com/dz/annuaire/industrie-pharmaceutique**"
      ],
      "metadata": {
        "id": "KQXQ7jvJkDTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to extract data from each company card\n",
        "def extract_data_from_card(card):\n",
        "    company_name = card.find('a', class_='stretched-link').text.strip()\n",
        "    industry = card.find('div', class_='text-14 text-brand-blue').text.strip()\n",
        "    address = card.find('address').text.strip()\n",
        "    telephone = card.find('a', class_='text-13').text.strip().split(':')[1].strip()\n",
        "    return [company_name, industry, address, telephone]\n",
        "\n",
        "# Send a GET request to the webpage\n",
        "url = 'https://www.goafricaonline.com/dz/annuaire/industrie-pharmaceutique'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract company information from each card\n",
        "    companies = []\n",
        "    company_cards = soup.find_all('article', class_='relative')\n",
        "    for card in company_cards:\n",
        "        company_data = extract_data_from_card(card)\n",
        "        companies.append(company_data)\n",
        "\n",
        "    # Write the extracted information to a CSV file\n",
        "    with open('pharmaceutical_companies.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Company Name', 'Industry', 'Address', 'Telephone'])\n",
        "        writer.writerows(companies)\n",
        "\n",
        "    print(\"Data written to pharmaceutical_companies.csv successfully.\")\n",
        "else:\n",
        "    print(f\"Failed to retrieve the webpage: {url}\")"
      ],
      "metadata": {
        "id": "n2A8Z6qVkKAk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}